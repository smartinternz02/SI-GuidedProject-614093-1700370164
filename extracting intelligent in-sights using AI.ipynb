{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IemUgkfAWJ8x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv(\"/content/news_summary.csv\", encoding='ISO-8859-1')\n",
        "data2 = pd.read_csv(\"/content/news_summary_more.csv\")"
      ],
      "metadata": {
        "id": "huOFMM0SWWuS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "da12f659-ee16-48b6-9af8-3d55f1d1bce9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-85910d2a6efd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/news_summary.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/news_summary_more.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/news_summary.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Kgw6m08mrYZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([data1, data2], axis = 0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FDW7r24PWh8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "SKWoyFWTXdyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "-jIm65m9bcIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "EDTh2vs9dKTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "0iTY1it6YgKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StopWords = set(stopwords.words('english'))\n",
        "def preprocess(text):\n",
        "    text = text.lower() #converting input to lowercase\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text) #Removing punctuations and special characters.\n",
        "    text = re.sub('\"','', text) #Removing double quotes.\n",
        "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #Replacing contractions.\n",
        "    text = re.sub(r\"'s\\b\",\"\",text) #Eliminating apostrophe.\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text) #Removing non-alphabetical characters\n",
        "    text = ' '.join([word for word in text.split() if word not in StopWords]) #Removing stopwords.\n",
        "    text = ' '.join([word for word in text.split() if len(word) >= 3]) #Removing very short words\n",
        "    return text"
      ],
      "metadata": {
        "id": "VMsNZpgpdSJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "cleaned_summary = []\n",
        "for text in data['text']:\n",
        "    cleaned_text.append(preprocess(text))\n",
        "for summary in data['headlines']:\n",
        "    cleaned_summary.append(preprocess(summary))\n",
        "cleaned_data = pd.DataFrame()\n",
        "cleaned_data['text'] = cleaned_text\n",
        "cleaned_data['headline'] = cleaned_summary\n",
        "\n",
        "#Replacing empty string summaries with nan values and then dropping those datapoints.\n",
        "cleaned_data['headline'].replace('', np.nan, inplace=True)\n",
        "cleaned_data.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "30ZE7_UydbNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data['headline'] = cleaned_data['headline'].apply(lambda x: '<START>' + ' '+ x + ' '+ '<END>')\n",
        "for i in range(10):\n",
        "    print('Article: ', cleaned_data['text'][i])\n",
        "    print('Headline:', cleaned_data['headline'][i])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "xe_AlWrfdtxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_length = max([len(text.split()) for text in cleaned_data['text']])\n",
        "headline_length = max([len(text.split()) for text in cleaned_data['headline']])\n",
        "print(news_length, headline_length)"
      ],
      "metadata": {
        "id": "2UM5pPUhdyWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "headline_word_count = []\n",
        "\n",
        "for i in cleaned_data['text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in cleaned_data['headline']:\n",
        "      headline_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Body':text_word_count, 'Highlights':headline_word_count})\n",
        "length_df.hist(bins = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qUkvBJ3Ad2rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpiR_YxBfi4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "YVPTJLVsWlXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_text'] = data['headlines'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "R11CtncfWlod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = ' '.join(data['processed_text']).split()\n",
        "word_freq = nltk.FreqDist(all_words)"
      ],
      "metadata": {
        "id": "SNv_5R2KWq8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, TimeDistributed, LSTM, Embedding, Input\n",
        "from keras import Model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "iHrLdIqsflKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['processed_text']\n",
        "y = data['text']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "odakcc4vXAVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_news = Tokenizer()\n",
        "tokenizer_news.fit_on_texts(list(X_train))\n",
        "x_train_seq = tokenizer_news.texts_to_sequences(X_train)\n",
        "x_test_seq = tokenizer_news.texts_to_sequences(X_test)\n",
        "#padding short texts with 0s.\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=news_length, padding='post')\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=news_length, padding='post')\n",
        "#Vocab size of texts.\n",
        "news_vocab = len(tokenizer_news.word_index) + 1\n",
        "\n",
        "#Keras Tokenizer for summaries.\n",
        "tokenizer_headline = Tokenizer()\n",
        "tokenizer_headline.fit_on_texts(list(y_train))\n",
        "y_train_seq = tokenizer_headline.texts_to_sequences(y_train)\n",
        "y_test_seq = tokenizer_headline.texts_to_sequences(y_test)\n",
        "y_train_pad = pad_sequences(y_train_seq, maxlen=headline_length, padding='post')\n",
        "y_test_pad = pad_sequences(y_test_seq, maxlen=headline_length, padding='post')\n",
        "#Vocab size of summaries.\n",
        "headline_vocab = len(tokenizer_headline.word_index) + 1"
      ],
      "metadata": {
        "id": "gb52uwPGftz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs): #-----> to inherit the layer class from keras\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):#------> function to create trainable weights w1,w2,w3\n",
        "        assert isinstance(input_shape, list)\n",
        "        self.W1 = self.add_weight(name='W1',shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),initializer='uniform',trainable=True)\n",
        "        self.W2 = self.add_weight(name='w2',shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),initializer='uniform',trainable=True)\n",
        "        self.W3 = self.add_weight(name='w3',shape=tf.TensorShape((input_shape[0][2], 1)),initializer='uniform',trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "\n",
        "        #inputs for this function: [encoder_output_sequence, decoder_output_sequence]\n",
        "        #outputs from this function : energy_i which is energy at step i\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):#----->function for computing energy for a single decoder state\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            #shaping tensors\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            #Computing S.W1 ------> dot product of encoder output with trainable weight w1\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            W1_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W1), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W1_dot_s.shape)\n",
        "\n",
        "            #Computing h.w2 -------> dot product of decoder with w2 weights\n",
        "            W2_dot_h = K.expand_dims(K.dot(inputs, self.W2), 1)\n",
        "            if verbose:\n",
        "                print('W2.h>',W2_dot_h.shape)\n",
        "\n",
        "            #tanh(S.W1a + hj.W2a)----> concatenation based attention score calculation\\\n",
        "            # tanh activation will squash the values to -1 to 1 range\n",
        "            reshaped_W1s_plus_W2h = K.tanh(K.reshape(W1_dot_s + W2_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_W1s_plus_W2h.shape)\n",
        "\n",
        "            #softmax(w3.tanh(S.W1 + hj.w2)) -----> probability distribution for energy\n",
        "            # calculating the attention energy at each step and dot producting with w3\n",
        "            energy_i = K.reshape(K.dot(reshaped_W1s_plus_W2h, self.W3), (-1, en_seq_len))\n",
        "            energy_i = K.softmax(energy_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('energy at i>', energy_i.shape)\n",
        "\n",
        "            return energy_i, [energy_i]\n",
        "\n",
        "        def context_step(inputs, states):#------> function to calculate context vector at each stage\n",
        "            #this function outputs the context vector at each stage\n",
        "            context_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return context_i, [context_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])\n",
        "            fake_state = K.expand_dims(fake_state)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])\n",
        "\n",
        "        #Computing energy outputs\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],)\n",
        "\n",
        "        #Computing context vectors\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],)\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #reshaping Outputs produced by the layer\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "metadata": {
        "id": "CGepdUjXf6_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "embedding_dim = 300 #Size of word embeddings.\n",
        "latent_dim = 500 #Number of neurons in LSTM layer.\n",
        "\n",
        "#Embedding Layer\n",
        "e_input = Input(shape=(news_length, ))\n",
        "e_emb = Embedding(news_vocab, embedding_dim, trainable=True)(e_input)\n",
        "\n",
        "#Three LSTM layers ----> encoder.\n",
        "e_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "y_1, a_1, c_1 = e_lstm1(e_emb)\n",
        "e_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "y_2, a_2, c_2 = e_lstm2(y_1)\n",
        "e_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "encoder_output, a_enc, c_enc = e_lstm3(y_2)\n",
        "\n",
        "#Single LSTM layer ----> decoder\n",
        "d_input = Input(shape=(None,))\n",
        "d_emb = Embedding(headline_vocab, embedding_dim, trainable=True)(d_input)\n",
        "d_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
        "decoder_output, decoder_fwd, decoder_back = d_lstm(d_emb, initial_state=[a_enc, c_enc]) #Final output states of encoder last layer are fed into decoder.\n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_output, decoder_output])\n",
        "\n",
        "#concatenating decoder input to attention layer output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_output, attn_out])\n",
        "#dense time distributed layer with softw=max fucntion for predicting the next word\n",
        "decoder_dense = TimeDistributed(Dense(headline_vocab, activation='softmax'))\n",
        "decoder_output = decoder_dense(decoder_concat_input)\n",
        "\n",
        "#creating model\n",
        "model = Model([e_input, d_input], decoder_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0V7BSZkjgo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "history=model.fit([x_train_pad,y_train_pad[:,:-1]], y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,epochs=3,callbacks=[callback],batch_size=512, validation_data=([x_test_pad,y_test_pad[:,:-1]], y_test_pad.reshape(y_test_pad.shape[0],y_test_pad.shape[1], 1)[:,1:]))"
      ],
      "metadata": {
        "id": "1IeucIFehf-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(inputs=e_input, outputs=[encoder_output, a_enc, c_enc])\n",
        "\n",
        "#Initialising state vectors for decoder.\n",
        "decoder_initial_state_a = Input(shape=(latent_dim,))\n",
        "decoder_initial_state_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state = Input(shape=(news_length, latent_dim))\n",
        "\n",
        "#Decoder inference model\n",
        "decoder_out, decoder_a, decoder_c = d_lstm(d_emb, initial_state=[decoder_initial_state_a, decoder_initial_state_c])\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state, decoder_out])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_out, attn_out_inf])\n",
        "\n",
        "decoder_final = decoder_dense(decoder_inf_concat)\n",
        "decoder_model = Model([d_input]+[decoder_hidden_state, decoder_initial_state_a, decoder_initial_state_c], [decoder_final]+[decoder_a, decoder_c])"
      ],
      "metadata": {
        "id": "yUMxEvkJhsP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoded_sequence(input_seq):\n",
        "    encoder_out, encoder_a, encoder_c = encoder_model.predict(input_seq) #Collecting output from encoder inference model.\n",
        "    #Initialise input to decoder neuron with START token. Thereafter output token predicted by each neuron will be used as input for the subsequent.\n",
        "    #Single elt matrix used for maintaining dimensions.\n",
        "    next_input = np.zeros((1,1))\n",
        "    next_input[0,0] = tokenizer_headline.word_index['start']\n",
        "    output_seq = ''\n",
        "    #Stopping condition to terminate loop when one summary is generated.\n",
        "    stop = False\n",
        "    while not stop:\n",
        "        #Output from decoder inference model, with output states of encoder used for initialisation.\n",
        "        decoded_out, trans_state_a, trans_state_c = decoder_model.predict([next_input] + [encoder_out, encoder_a, encoder_c])\n",
        "        #Get index of output token from y(t) of decoder.\n",
        "        output_idx = np.argmax(decoded_out[0, -1, :])\n",
        "        #If output index corresponds to END token, summary is terminated without of course adding the END token itself.\n",
        "        if output_idx == tokenizer_headline.word_index['end']:\n",
        "            stop = True\n",
        "        elif output_idx>0 and output_idx != tokenizer_headline.word_index['start'] :\n",
        "            output_token = tokenizer_headline.index_word[output_idx] #Generate the token from index.\n",
        "            output_seq = output_seq + ' ' + output_token #Append to summary\n",
        "\n",
        "        #Pass the current output index as input to next neuron.\n",
        "        next_input[0,0] = output_idx\n",
        "        #Continously update the transient state vectors in decoder.\n",
        "        encoder_a, encoder_c = trans_state_a, trans_state_c\n",
        "\n",
        "    return output_seq"
      ],
      "metadata": {
        "id": "6c82j6pShwnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = []\n",
        "for i in range(20):\n",
        "    print('Information:', X_test.iloc[i])\n",
        "    print('Actual Headline:', y_test.iloc[i])\n",
        "    print('Predicted Headline:', decoded_sequence(x_test_pad[i].reshape(1, news_length)))\n",
        "    predicted.append(decoded_sequence(x_test_pad[i].reshape(1, news_length)).split())"
      ],
      "metadata": {
        "id": "WK4agVQqh1TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.colheader_justify', 'right')\n",
        "df = pd.DataFrame(columns=['Information', 'Actual Headline', 'Predicted Headline'])\n",
        "\n",
        "for i in range(10):\n",
        "    information = X_test.iloc[i]\n",
        "    actual_headline = y_test.iloc[i]\n",
        "    predicted_headline = decoded_sequence(x_test_pad[i].reshape(1, news_length))\n",
        "    predicted.append(predicted_headline.split())\n",
        "\n",
        "    row = {'Information': information, 'Actual Headline': actual_headline, 'Predicted Headline': predicted_headline}\n",
        "    df = df.append(row, ignore_index=True)\n",
        "\n",
        "display(df)\n"
      ],
      "metadata": {
        "id": "IHAf1jrLh8Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000  # Adjust based on vocabulary size\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "2ISFHOabXDwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n"
      ],
      "metadata": {
        "id": "nWnFxAMuXEqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100  # Maximum sequence length\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Building LSTM model"
      ],
      "metadata": {
        "id": "mjRNUEeyXIED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "wCWe8RolXON6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique labels in y_train and y_test\n",
        "print(\"Unique labels in y_train:\", np.unique(y_train))\n",
        "print(\"Unique labels in y_test:\", np.unique(y_test))\n",
        "\n",
        "# Identify the problematic label causing the KeyError\n",
        "# Once identified, filter the data to exclude rows with this label\n",
        "problematic_label = 'The first humans to land on the Moon, Neil Armstrong and Buzz Aldrin...'  # Replace this with the problematic label causing the KeyError\n",
        "\n",
        "# Filter the data to exclude rows with the problematic label\n",
        "train_mask = y_train != problematic_label\n",
        "test_mask = y_test != problematic_label\n",
        "\n",
        "X_train_filtered = X_train[train_mask]\n",
        "y_train_filtered = y_train[train_mask]\n",
        "\n",
        "X_test_filtered = X_test[test_mask]\n",
        "y_test_filtered = y_test[test_mask]\n",
        "\n",
        "# Proceed with model training using the filtered data\n"
      ],
      "metadata": {
        "id": "xaaGCQjLZdcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Combine the training and test labels to ensure consistent encoding\n",
        "combined_labels = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Fit the LabelEncoder on combined labels\n",
        "le = LabelEncoder()\n",
        "le.fit(combined_labels)\n",
        "\n",
        "# Transform both y_train and y_test using the fitted encoder\n",
        "y_train_encoded = le.transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Proceed with model training using the encoded labels\n"
      ],
      "metadata": {
        "id": "YAliyKYZcFrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Mapping descriptive labels to categorical labels\n",
        "label_mapping = {\n",
        "    'label_description_1': 'category_1',\n",
        "    'label_description_2': 'category_2',\n",
        "    # Add more mappings as needed\n",
        "}\n",
        "\n",
        "# Apply mapping to convert descriptive labels to categorical labels\n",
        "y_train_categorical = y_train.map(label_mapping)\n",
        "y_test_categorical = y_test.map(label_mapping)\n",
        "\n",
        "# Then, use LabelEncoder on the transformed categorical labels\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train_categorical)\n",
        "\n",
        "# Transform both y_train and y_test using the fitted encoder\n",
        "y_train_encoded = le.transform(y_train_categorical)\n",
        "y_test_encoded = le.transform(y_test_categorical)\n",
        "\n",
        "# Proceed with model training using the encoded labels\n"
      ],
      "metadata": {
        "id": "VjS6pI-CciG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Fit LabelEncoder separately on training and test labels\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)  # Fit on y_train labels\n",
        "\n",
        "# Transform both y_train and y_test using the fitted encoder\n",
        "y_train_encoded = le.transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Proceed with model training using the encoded labels\n"
      ],
      "metadata": {
        "id": "y4UNsuYAcR1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'y_train' and 'y_test' contain string labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Rest of the code remains the same\n"
      ],
      "metadata": {
        "id": "CldfTwSxZHzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "A26p9BzdXO5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_test_padded, y_test), callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "l5oYtlCyXRQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "nVp2ZjZHXWO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}